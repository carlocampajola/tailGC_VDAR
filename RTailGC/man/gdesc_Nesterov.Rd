% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nesterov_gd.R
\name{gdesc_Nesterov}
\alias{gdesc_Nesterov}
\title{Nesterov Gradient Descent}
\usage{
gdesc_Nesterov(
  par,
  fn,
  gr,
  ...,
  lower = NA,
  upper = NA,
  lr = 0.01,
  maxiter = 100,
  verbose = T,
  grad_tol = 5e-04,
  n_small_grad = 10,
  deltaf_tol = 5e-04
)
}
\arguments{
\item{par}{A numeric vector with the starting values of the parameters.}

\item{fn}{A function returning the value of the cost function to be minimized taking as first argument the vector of parameters in par.}

\item{gr}{A function returning the gradient of the cost function taking as first argument the vector of parameters in par.}

\item{...}{Additional arguments for fn and gr.}

\item{lower}{A scalar indicating a lower bound for the parameters in par. NA implies no lower bound. Defaults to NA.}

\item{upper}{Same as lower, but for the upper bound. Defaults to NA.}

\item{lr}{The learning rate of the algorithm. Defaults to 0.01.}

\item{maxiter}{Maximum number of gradient descent steps. Defaults to 100.}

\item{verbose}{Whether progress should be reported with text messages. Defaults to TRUE.}

\item{grad_tol}{threshold for the modulus of the gradient below which the algorithm stops after n_small_grad iterations. Defaults to 0.0005.}

\item{n_small_grad}{Number of iterations after which to stop if gradient modulus falls below the grad_tol level. Defaults to 10.}

\item{deltaf_tol}{Threshold for the relative decrease of the cost function below which the algorithm stops. Defaults to 0.0005.}
}
\value{
A list containing an argument \code{par} with the optimized values of the parameters and an argument \code{value} with the value of the objective function at the optimized coordinates.
}
\description{
This is a generic implementation of the Nesterov accelerated gradient descent algorithm. It takes a vector of parameters par as input, together with a function fn to be minimized and its gradient gr. Optional arguments are the lower and upper bounds for the par values, the learning rate lr, maximum number of iterations, stopping criteria and whether verbose reporting should be enabled.
}
